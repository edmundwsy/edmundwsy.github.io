<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Task Planning with PDDL Inference for TIAGo Robot | Siyuan Wu</title> <meta name="author" content="Siyuan Wu"> <meta name="description" content="1. The motion planning part is based on MoveIt and teb_local_planner &lt;br&gt; 2. The object perception is achieved by detecting AprilTag markers on the objects &lt;br&gt; 3. The planning domain definition language is applied for high-level reasoning and task planning &lt;br&gt; 4. The demonstration in Gazebo simulation is finished as the course project of &lt;b&gt;Knowledge Representation and Symbolic Reasoning&lt;/b&gt; at TU Delft. &lt;br&gt; 5. The real-world demonstration is achieved as parts of the &lt;b&gt;Mulitdisciplinary Project&lt;/b&gt;"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700%7CRoboto+Slab:100,300,400,500,700%7CMaterial+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%9A%80&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="siyuanwu99.github.io/projects/ro47007-mdp/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">Siyuan Wu</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Task Planning with PDDL Inference for TIAGo Robot</h1> </header> <article> <h2 id="simulation-demo">Simulation demo</h2> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/krr-diagram-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/krr-diagram-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/krr-diagram-1400.webp"></source> <img src="/assets/img/krr-diagram.png" class="img-fluid rounded z-depth-1" width="640" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <center> <video controls="" autoplay="true" width="640" zoomable="true"> <source src="/assets/img/krr-gazebo.mp4" type="video/mp4"></source> Download the <a href="/assets/img/krr-gazebo.mp4">MP4</a> video. </video> </center> <h2 id="real-world-demo">Real-world demo</h2> <center> <video controls="" autoplay="true" width="640" zoomable="true"> <source src="/assets/img/47007-mdp/ro47007.mp4" type="video/mp4"></source> Download the <a href="/assets/img/47007-mdp/ro47007.mp4">MP4</a> video. </video> </center> <h2 id="documentation">Documentation</h2> <h3 id="navigation">Navigation</h3> <p>Navigation consists of driving the robot via the found path by the path planner to the goal. Most navigation is already integrated with the provided ROS packages for the TIAGo. In this project, a node is added called "navigation_point_direction" that will point the head towards the predicted direction the robot will drive. This way, humans around the robot will better understand where the robot will drive, thus making it safer. The second advantage is that the head will look to the new driving direction a bit in advance. This way, the sensors can detect obstacles earlier, and thereby the TIAGo can plan more efficiently.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/47007-mdp/navigation_point_direction_simulation-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/47007-mdp/navigation_point_direction_simulation-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/47007-mdp/navigation_point_direction_simulation-1400.webp"></source> <img src="/assets/img/47007-mdp/navigation_point_direction_simulation.png" class="img-fluid rounded z-depth-1" width="320" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>In the existing packages, a node publishes a vector of the predicted driving direction, and the Point Direction node reads this vector. The value is calculated to the head’s pitch and yaw rotation values. When the robot makes a sharp turn left or right, without moving forward, the head will look down and in that direction, as can be seen in figure [2]. The more the robot will drive forward and not turn, the more the head will look straight ahead. The head rotation is limited to +-70 degrees left and right, and +-50 degrees up and down. The calculation is robust to changing the sign of the angle, so if the vector points left, the head will always look to the left and visa versa. If the vector crosses 180 degrees, to the back, then the head will fast rotate from one side 70 degrees to the other side 70 degrees. In the current ROS program that will never happen as the robot never drives backwards, only rotation and forward. The head orientation is published to the head_controller topic, ensuring the head will move in the correct directions. The head controller action server is not used as this control is not crucial or time-dependent for other TIAGo functionalities. In the simulation, moving the head works fine when the robot drives and rotates. When the prediction vector is also plotted in RViz (as a green arrow), it is visible that the head rotations copy the behavior of the vector.</p> <h3 id="path-planning">Path Planning</h3> <p>Path planning is to generate a collision-free trajectory from TIAGo’s current position to the goal. To avoid a collision, the robot should be able to estimate obstacles’ shape and position and build an occupancy map for the planner. In the package, a static occupancy map is created from laser data sampled by onboard laser and RGB-D camera. It can provide an accurate perception of static obstacles; however, it fails to work on dynamic obstacles. In our case, visitors are walking around TIAGo, so dynamic obstacle avoidance is essential. We implemented a simple obstacle extractor and tracker to estimate the position and velocity of these obstacles. First, laser inputs are converted to point cloud. Then, clustering is applied to these points to extract line segments and circles. Since point segments are only caused by walls or furniture, we discard them and only keep the circles. The radius of circles are obtained in this step. Then, an Extended Kalman Filter (EKF) is applied to these clusters to track their movement and estimate their accurate position and velocity. These observations are published to <code class="language-plaintext highlighter-rouge">/obstacle_observer/obstacles</code> topic whose message type is defined in the obstacle_observer package.</p> <p>We use the default A* for global planning and an open-source package <code class="language-plaintext highlighter-rouge">teb_local_planner</code> for local planning. The goal of a global planner is to find a discrete path with colliding wall and table. For the local planner, it should refine the path considering the nonholonomic constraints of the robot base and avoid dynamic obstacles. The <code class="language-plaintext highlighter-rouge">teb_local_planner</code> package enables these functions <sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>, but the input message type has to be <code class="language-plaintext highlighter-rouge">ObstacleArrayMsg</code> from costmap_converter package. We implemented a dynamic_obs_node as a bridge between two packages. It subscribes to <code class="language-plaintext highlighter-rouge">/obstacle_observer/obstacles</code> and publish to <code class="language-plaintext highlighter-rouge">/move_base/TebLocalPlannerROS/obstacles</code>.</p> <p>Although the idea seemed good, after implementation we found that the algorithm was not robust. In simulated tests, the laser detection either missed the visitor or treated the chair as a visitor, regardless of the parameters we set. This success rate was too low (&lt; 30%) to be used in the final system. In the end we decided to use <code class="language-plaintext highlighter-rouge">move_base</code> package and only consider static obstacles.</p> <h3 id="picking-sequence">Picking Sequence</h3> <p>The main purpose of the picking sequence is to execute several robot actions as a whole, e.g. search cups, pick cups, move to the recycling bin, and place cups inside the bin. All robot behaviors related to the cup are encapsulated in this ROS node. These behaviors are implemented by ROS action servers. This /sequence_node will run the ROS action client corresponding to the standalone functions. It is launched when the user presses the "searching cups" button on GUI. Once started, it will first run a search client to find the nearest coffee cups by AprilTag markers on the cup. Next, the robot will run a move client to approach the detected coffee cup and prepare to pick. Then, a pick client is called to prepare the picking action and finally pick the detected coffee cup. If the cup is successfully picked, another move client is called to drive the robot to the recycling bin. Finally, the robot will run a place client to place the cup inside the bin. Once the program was interrupted with some errors, e.g. TIAGo failed to pick the cup, or no cups were found, the <code class="language-plaintext highlighter-rouge">/sequence_node</code> will terminate with some error code.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/47007-mdp/search_server-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/47007-mdp/search_server-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/47007-mdp/search_server-1400.webp"></source> <img src="/assets/img/47007-mdp/search_server.png" class="img-fluid rounded z-depth-1" width="320" height="auto" alt="TIAGo approaches coffee cup. Computed waypoint is marked with red long arrow." data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h4 id="search-server">Search server</h4> <p>Search Server is a ROS action server we build from scratch for searching coffee cups. It uses a self-defined message type named as <code class="language-plaintext highlighter-rouge">Search.action</code> for communication between server and client. Similar to other action messages, it consists of a goal, a result, and feedback. The goal is an integer that indicates the number of objects to search for. The result contains a boolean variable representing success or failure and an integer variable representing the ID of the detected AprilTag marker. This ROS action server subscribes to <code class="language-plaintext highlighter-rouge">/tag_detection</code> provided by the existing AprilTag_ros package. It maintains a ROS service waiting to be called by the ROS action client. Once called, the server will execute commands in 3 stages. First, TIAGo will scan the entire area by turning its head and looking around. Simultaneously, TIAGo will receive and analyze detection results from <code class="language-plaintext highlighter-rouge">/tag_detection</code> and save the position and orientation of one selected AprilTag marker. Then, the robot will apply a homogeneous transformation to compute a collision-free waypoint in front of the marker in the map frame. This waypoint is set to be 0.78 meters ahead of the coffee cup to be picked. Finally, TIAGo will send this waypoint to move_base and will move to it.</p> <h4 id="pick-server">Pick server</h4> <p>is a ROS action server for path planning in cup picking. It utilizes moveit<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup>, an open-source project for robot arm planning. Similar to Search Server, it subscribes to <code class="language-plaintext highlighter-rouge">/tag_detections</code> topic as well. It also subscribes to several ROS service, e.g. <code class="language-plaintext highlighter-rouge">add_collision_object</code> and <code class="language-plaintext highlighter-rouge">remove_collision_object</code> to build and clean the occupancy map. This map helps to find a collision-free trajectory for the TIAGo gripper to grasp the cup. Another ROS server it subscribes to is <code class="language-plaintext highlighter-rouge">get_grasp_pose</code>, which provides the accurate grasp position when the robot is working on a coffee cup.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/47007-mdp/poses-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/47007-mdp/poses-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/47007-mdp/poses-1400.webp"></source> <img src="/assets/img/47007-mdp/poses.png" class="img-fluid rounded z-depth-1" width="320" height="auto" alt="Settings of three grasp poses: pre-grasp pose (red), grasp pose (yellow) and post-grasp pose (green). The post pose is 10 centimeters above the pre pose." data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>The pick server should receive requests from the pick client in the sequence_node. The request should contain the exact ID of the coffee cup detected by the search server The entire picking process can be separated into eight states.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/47007-mdp/pre_grasp_pose-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/47007-mdp/pre_grasp_pose-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/47007-mdp/pre_grasp_pose-1400.webp"></source> <img src="/assets/img/47007-mdp/pre_grasp_pose.png" class="img-fluid rounded z-depth-1" width="480" height="auto" alt="Pre-grasp pose in the simulation" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/47007-mdp/grasp_pose-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/47007-mdp/grasp_pose-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/47007-mdp/grasp_pose-1400.webp"></source> <img src="/assets/img/47007-mdp/grasp_pose.png" class="img-fluid rounded z-depth-1" width="480" height="auto" alt="Grasp pose in the simulation" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/47007-mdp/post_grasp_pose-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/47007-mdp/post_grasp_pose-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/47007-mdp/post_grasp_pose-1400.webp"></source> <img src="/assets/img/47007-mdp/post_grasp_pose.png" class="img-fluid rounded z-depth-1" width="480" height="auto" alt="Post-grasp pose in the simulation" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ol> <li> <p><strong>Look around</strong>. TIAGo’s head iterate from 7 preset poses, covering all areas it can see to find and locate the request AprilTag marker.</p> </li> <li> <p><strong>Add collision objects</strong>. TIAGo utilizes its RGB-D camera to build an occupancy map including cups and the table.</p> </li> <li> <p><strong>Compute grasp position</strong>. Based on the position and orientation of the AprilTag marker in the camera frame, TIAGo computes the gripper’s grasp poses and transforms it into the base frame. The grasp pose is the pose of the gripper to pick the object. Pre-grasp pose and post-grasp pose should be easy to reach safely.</p> </li> <li> <p><strong>Move to pre-grasp pose</strong>. TIAGo first raises its right arm, then slowly moves the gripper to the pre-grasp position.</p> </li> <li> <p><strong>Move to grasp pose</strong>. TIAGo puts its gripper out to the cup.</p> </li> <li> <p><strong>Close the gripper and attach the cup</strong>.</p> </li> <li> <p><strong>Move to post-grasp pose</strong>.</p> </li> <li> <p><strong>Remove collision objects</strong>. The occupancy map is cleaned, and robots can move to other positions</p> </li> </ol> <h4 id="place-server">Place Server</h4> <p>This part contains the ROS action server for placing the coffee cup in the recycling bin. Due to the location of the bin being fixed, we do not need any AprilTag marker to tell TIAGo where the gripper should move. We use a fixed gripper pose hard-coded in <code class="language-plaintext highlighter-rouge">/sequence_node</code>, sent to the place server as a goal. The gripper pose is 0.60 meters in front of the robot, 0.10 meters to the right, and 0.4 meters high in the base_link frame. After tuning in the simulation, we found that this is the best parameter setting for placing cups into the bin. Similar to picking, the place server can be divided into three steps. First, TIAGo looks around and builds an occupancy map of the environment. Second, TIAGo moves its arm to the placing pose and opens the gripper to release the coffee cup. Finally, TIAGo tucks its arms and clears the occupancy map.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/47007-mdp/place-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/47007-mdp/place-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/47007-mdp/place-1400.webp"></source> <img src="/assets/img/47007-mdp/place.png" class="img-fluid rounded z-depth-1" width="360" height="auto" alt="Place pose (purple) of the gripper" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h4 id="performance">Performance</h4> <p>We test the performance of search-pick-place sequence in the simulation environment. We modified the position and orientation of the coffee cup and started the robot in different positions to test the performance of our robot in different environments.</p> <p>As shown in the table, our node reaches 60% success rate in search, pick and place task. As for search server, the success rate is 100%. This is because it utilizes the AprilTag marker, which guarantees its stability and accuracy. For pick server, the success rate is 60%, which is the lowest among three implemented servers. For place server, the success rate is 100%. Due to we use a fixed position of the coffee bins and there are no obstacles between the table and the bin, placing task is relatively easy. It’s obvious that if pick server succeeds, the place server will succeed.</p> <table class="table table-sm"> <tr> <td>Number</td> <td>Search</td> <td>Pick</td> <td>Place</td> </tr> <tr> <td>01</td> <td>✅</td> <td>failed: grasp</td> <td></td> </tr> <tr> <td>02</td> <td>✅</td> <td>failed: pre-grasp</td> <td></td> </tr> <tr> <td>03</td> <td>✅</td> <td>✅</td> <td>✅</td> </tr> <tr> <td>04</td> <td>✅</td> <td>✅</td> <td>✅</td> </tr> <tr> <td>05</td> <td>✅</td> <td>✅</td> <td>✅</td> </tr> <tr> <td>06</td> <td>✅</td> <td>failed: pre-grasp</td> <td></td> </tr> <tr> <td>07</td> <td>✅</td> <td>✅</td> <td>✅</td> </tr> <tr> <td>08</td> <td>✅</td> <td>failed: grasp</td> <td></td> </tr> <tr> <td>09</td> <td>✅</td> <td>✅</td> <td>✅</td> </tr> <tr> <td>10</td> <td>✅</td> <td>✅</td> <td>✅</td> </tr> </table> <p>Experiments in simulation environment</p> <p>As indicated in the table, pick server is the bottleneck of this node. The failure reason of pick server is complex. For test 01, 02, 06, picking failed because the orientation is not suitable. In our node, we only managed to use the right gripper to pick objects, and we use the orientation of AprilTag markers to compute the pre-grasp and grasp pose. Therefore, when the AprilTag marker is orienting to the left of the robot , the robot may not find a feasible trajectory to move the cup, and thus, picking will fail.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/47007-mdp/bad_pick_02-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/47007-mdp/bad_pick_02-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/47007-mdp/bad_pick_02-1400.webp"></source> <img src="/assets/img/47007-mdp/bad_pick_02.png" class="img-fluid rounded z-depth-1" width="360" height="auto" alt="Failed picking: orientation not suitable" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/47007-mdp/bad_pick_03-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/47007-mdp/bad_pick_03-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/47007-mdp/bad_pick_03-1400.webp"></source> <img src="/assets/img/47007-mdp/bad_pick_03.png" class="img-fluid rounded z-depth-1" width="360" height="auto" alt="Failed picking: too far to reach" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>For test 08, the distance between the robot and the coffee cup is too large. Therefore, robot’s gripper cannot reach the cup when it fully strengthens its arm. Although it has already reached the pre-grasp pose, however, it has some problem to reach the grasp pose so that picking failed.</p> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1" role="doc-endnote"> <p><a href="http://wiki.ros.org/teb_local_planner/Tutorials/Track%20and%20include%20dynamic%20obstacles%20via%20costmap_converter" rel="external nofollow noopener" target="_blank">http://wiki.ros.org/teb_local_planner/Tutorials/Track%20and%20include%20dynamic%20obstacles%20via%20costmap_converter</a> <a href="#fnref:1" class="reversefootnote" role="doc-backlink">↩</a></p> </li> <li id="fn:2" role="doc-endnote"> <p><a href="https://moveit.ros.org/" rel="external nofollow noopener" target="_blank">https://moveit.ros.org/</a> <a href="#fnref:2" class="reversefootnote" role="doc-backlink">↩</a></p> </li> </ol> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Siyuan Wu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: April 04, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://cdn.panelbear.com/analytics.js?site=141v6hERZVq"></script> <script>window.panelbear=window.panelbear||function(){(window.panelbear.q=window.panelbear.q||[]).push(arguments)},panelbear("config",{site:"141v6hERZVq"});</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>